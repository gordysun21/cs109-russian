"""
Logistic Regression Explanation for Bot Detection Demo

This script provides a comprehensive explanation of the mathematical concepts
behind logistic regression used in our bot detection system.

Topics covered:
1. Logistic Regression Fundamentals
2. Sigmoid Function
3. Maximum Likelihood Estimation (MLE)
4. Gradient Descent/Ascent
5. Application to Bot Detection
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.optimize import minimize
import seaborn as sns

class LogisticRegressionExplainer:
    def __init__(self):
        self.setup_style()
        
    def setup_style(self):
        """Setup plotting style"""
        plt.style.use('default')
        sns.set_palette("husl")
        
    def explain_all_concepts(self):
        """Run complete explanation of all concepts"""
        print("ðŸ¤– LOGISTIC REGRESSION FOR BOT DETECTION")
        print("=" * 60)
        print()
        
        # 1. Basic concept explanation
        self.explain_basic_concept()
        
        # 2. Sigmoid function
        self.explain_sigmoid_function()
        
        # 3. Maximum Likelihood Estimation
        self.explain_mle()
        
        # 4. Gradient Descent
        self.explain_gradient_descent()
        
        # 5. Bot detection application
        self.explain_bot_detection_application()
        
        print("\nðŸŽ¯ SUMMARY")
        print("-" * 30)
        self.provide_summary()
        
    def explain_basic_concept(self):
        """Explain the basic concept of logistic regression"""
        print("ðŸ“š 1. LOGISTIC REGRESSION FUNDAMENTALS")
        print("-" * 45)
        print()
        
        print("ðŸ”¹ What is Logistic Regression?")
        print("   Logistic regression is a statistical method for binary classification.")
        print("   Unlike linear regression, it predicts PROBABILITIES between 0 and 1.")
        print()
        
        print("ðŸ”¹ Why not Linear Regression for Classification?")
        print("   â€¢ Linear regression can predict values outside [0,1]")
        print("   â€¢ We need probabilities that are always between 0 and 1")
        print("   â€¢ Linear regression assumes constant variance (not true for binary outcomes)")
        print()
        
        print("ðŸ”¹ The Logistic Regression Model:")
        print("   Instead of: y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™")
        print("   We use:    P(y=1|x) = 1 / (1 + e^(-z))")
        print("   Where:     z = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™")
        print()
        
        print("ðŸ”¹ In Bot Detection Context:")
        print("   â€¢ y = 1 if account is a bot, 0 if human")
        print("   â€¢ xâ‚, xâ‚‚, ... xâ‚™ are features (followers, tweets, etc.)")
        print("   â€¢ P(y=1|x) is the probability that an account is a bot")
        print()
        
    def explain_sigmoid_function(self):
        """Explain and visualize the sigmoid function"""
        print("ðŸ“ˆ 2. THE SIGMOID FUNCTION")
        print("-" * 35)
        print()
        
        print("ðŸ”¹ Mathematical Definition:")
        print("   Ïƒ(z) = 1 / (1 + e^(-z))")
        print("   where z is the linear combination: z = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™")
        print()
        
        print("ðŸ”¹ Key Properties:")
        print("   â€¢ Always outputs values between 0 and 1")
        print("   â€¢ S-shaped (sigmoid) curve")
        print("   â€¢ Ïƒ(0) = 0.5 (decision boundary)")
        print("   â€¢ Ïƒ(+âˆž) â†’ 1, Ïƒ(-âˆž) â†’ 0")
        print("   â€¢ Smooth and differentiable everywhere")
        print()
        
        # Create visualization
        self.plot_sigmoid_function()
        
        print("ðŸ”¹ Interpretation in Bot Detection:")
        print("   â€¢ z > 0 â†’ P(bot) > 0.5 â†’ Classified as bot")
        print("   â€¢ z < 0 â†’ P(bot) < 0.5 â†’ Classified as human")
        print("   â€¢ z = 0 â†’ P(bot) = 0.5 â†’ Decision boundary")
        print()
        
    def plot_sigmoid_function(self):
        """Plot the sigmoid function with annotations"""
        z = np.linspace(-6, 6, 100)
        sigmoid = 1 / (1 + np.exp(-z))
        
        plt.figure(figsize=(12, 6))
        
        # Main sigmoid plot
        plt.subplot(1, 2, 1)
        plt.plot(z, sigmoid, 'b-', linewidth=3, label='Ïƒ(z) = 1/(1+e^(-z))')
        plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision Boundary (P=0.5)')
        plt.axvline(x=0, color='r', linestyle='--', alpha=0.7)
        plt.xlabel('z (linear combination)', fontsize=12)
        plt.ylabel('P(bot)', fontsize=12)
        plt.title('Sigmoid Function for Bot Detection', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # Add annotations
        plt.annotate('Human\n(P < 0.5)', xy=(-3, 0.1), fontsize=11, ha='center',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
        plt.annotate('Bot\n(P > 0.5)', xy=(3, 0.9), fontsize=11, ha='center',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral", alpha=0.7))
        
        # Derivative plot
        plt.subplot(1, 2, 2)
        sigmoid_derivative = sigmoid * (1 - sigmoid)
        plt.plot(z, sigmoid_derivative, 'g-', linewidth=3, label="Ïƒ'(z) = Ïƒ(z)(1-Ïƒ(z))")
        plt.xlabel('z', fontsize=12)
        plt.ylabel("Ïƒ'(z)", fontsize=12)
        plt.title('Sigmoid Derivative\n(used in gradient descent)', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        plt.tight_layout()
        plt.show()
        
    def explain_mle(self):
        """Explain Maximum Likelihood Estimation"""
        print("ðŸŽ¯ 3. MAXIMUM LIKELIHOOD ESTIMATION (MLE)")
        print("-" * 50)
        print()
        
        print("ðŸ”¹ What is MLE?")
        print("   MLE finds the parameter values (Î²â‚€, Î²â‚, ..., Î²â‚™) that make")
        print("   our observed data most likely to have occurred.")
        print()
        
        print("ðŸ”¹ The Likelihood Function:")
        print("   For a single observation i:")
        print("   L(Î²áµ¢) = P(yáµ¢=1|xáµ¢)^yáµ¢ Ã— P(yáµ¢=0|xáµ¢)^(1-yáµ¢)")
        print("   L(Î²áµ¢) = Ïƒ(záµ¢)^yáµ¢ Ã— (1-Ïƒ(záµ¢))^(1-yáµ¢)")
        print()
        
        print("ðŸ”¹ For all n observations:")
        print("   L(Î²) = âˆáµ¢â‚Œâ‚â¿ Ïƒ(záµ¢)^yáµ¢ Ã— (1-Ïƒ(záµ¢))^(1-yáµ¢)")
        print()
        
        print("ðŸ”¹ Log-Likelihood (easier to work with):")
        print("   â„“(Î²) = log(L(Î²)) = Î£áµ¢â‚Œâ‚â¿ [yáµ¢log(Ïƒ(záµ¢)) + (1-yáµ¢)log(1-Ïƒ(záµ¢))]")
        print()
        
        print("ðŸ”¹ In Bot Detection Context:")
        print("   â€¢ yáµ¢ = 1 if account i is a bot, 0 if human")
        print("   â€¢ Ïƒ(záµ¢) = predicted probability that account i is a bot")
        print("   â€¢ We want to maximize â„“(Î²) to find best parameters")
        print()
        
        # Demonstrate with example
        self.demonstrate_mle_example()
        
    def demonstrate_mle_example(self):
        """Demonstrate MLE with a simple example"""
        print("ðŸ”¹ Simple Example:")
        print("   Suppose we have 3 accounts with features and true labels:")
        print()
        
        # Create example data
        examples = pd.DataFrame({
            'Account': ['@user1', '@bot2', '@user3'],
            'Followers/Following': [2.5, 0.1, 8.0],
            'True Label': ['Human (0)', 'Bot (1)', 'Human (0)']
        })
        print(examples.to_string(index=False))
        print()
        
        print("   If our model predicts probabilities: [0.2, 0.9, 0.1]")
        print("   Likelihood = 0.8 Ã— 0.9 Ã— 0.9 = 0.648")
        print()
        print("   If our model predicts probabilities: [0.7, 0.9, 0.1]")
        print("   Likelihood = 0.3 Ã— 0.9 Ã— 0.9 = 0.243")
        print()
        print("   The first model is better! (Higher likelihood)")
        print()
        
    def explain_gradient_descent(self):
        """Explain gradient descent/ascent"""
        print("â¬‡ï¸ 4. GRADIENT DESCENT/ASCENT")
        print("-" * 40)
        print()
        
        print("ðŸ”¹ The Optimization Problem:")
        print("   We want to maximize log-likelihood â„“(Î²)")
        print("   Equivalently: minimize negative log-likelihood -â„“(Î²)")
        print()
        
        print("ðŸ”¹ Gradient Descent Algorithm:")
        print("   1. Start with random parameter values Î²â½â°â¾")
        print("   2. Compute gradient: âˆ‡â„“(Î²) = âˆ‚â„“/âˆ‚Î²")
        print("   3. Update: Î²â½áµ—âºÂ¹â¾ = Î²â½áµ—â¾ + Î±âˆ‡â„“(Î²â½áµ—â¾)  [ascent]")
        print("      Or:    Î²â½áµ—âºÂ¹â¾ = Î²â½áµ—â¾ - Î±âˆ‡(-â„“)(Î²â½áµ—â¾) [descent]")
        print("   4. Repeat until convergence")
        print()
        
        print("ðŸ”¹ The Gradient Formula:")
        print("   âˆ‚â„“/âˆ‚Î²â±¼ = Î£áµ¢â‚Œâ‚â¿ (yáµ¢ - Ïƒ(záµ¢)) Ã— xáµ¢â±¼")
        print("   Where:")
        print("   â€¢ yáµ¢ = true label (0 or 1)")
        print("   â€¢ Ïƒ(záµ¢) = predicted probability")
        print("   â€¢ xáµ¢â±¼ = feature j for observation i")
        print()
        
        print("ðŸ”¹ Intuition:")
        print("   â€¢ If yáµ¢ > Ïƒ(záµ¢): prediction too low â†’ increase Î²â±¼")
        print("   â€¢ If yáµ¢ < Ïƒ(záµ¢): prediction too high â†’ decrease Î²â±¼")
        print("   â€¢ Update magnitude proportional to feature value xáµ¢â±¼")
        print()
        
        # Visualize gradient descent
        self.visualize_gradient_descent()
        
    def visualize_gradient_descent(self):
        """Visualize gradient descent process"""
        # Simple 1D example
        def negative_log_likelihood(beta, x, y):
            z = beta * x
            p = 1 / (1 + np.exp(-z))
            p = np.clip(p, 1e-15, 1-1e-15)  # Avoid log(0)
            return -np.sum(y * np.log(p) + (1-y) * np.log(1-p))
        
        # Generate example data
        np.random.seed(42)
        x = np.array([1, 2, 3, 4, 5, 6])
        y = np.array([0, 0, 0, 1, 1, 1])
        
        # Plot likelihood surface
        betas = np.linspace(-2, 4, 100)
        likelihoods = [negative_log_likelihood(b, x, y) for b in betas]
        
        plt.figure(figsize=(12, 5))
        
        # Likelihood surface
        plt.subplot(1, 2, 1)
        plt.plot(betas, likelihoods, 'b-', linewidth=2)
        plt.xlabel('Î² (parameter value)', fontsize=12)
        plt.ylabel('Negative Log-Likelihood', fontsize=12)
        plt.title('Optimization Surface', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        
        # Mark minimum
        min_idx = np.argmin(likelihoods)
        plt.plot(betas[min_idx], likelihoods[min_idx], 'ro', markersize=8, label='Optimal Î²')
        plt.legend()
        
        # Gradient descent path
        plt.subplot(1, 2, 2)
        
        # Simulate gradient descent
        beta_path = []
        beta = -1.0  # Starting point
        learning_rate = 0.1
        
        for iteration in range(20):
            beta_path.append(beta)
            # Compute gradient
            z = beta * x
            p = 1 / (1 + np.exp(-z))
            gradient = np.sum((y - p) * x)
            # Update (gradient ascent on log-likelihood)
            beta = beta + learning_rate * gradient
            
        plt.plot(range(len(beta_path)), beta_path, 'ro-', linewidth=2, markersize=6)
        plt.axhline(y=betas[min_idx], color='g', linestyle='--', alpha=0.7, label='Optimal Î²')
        plt.xlabel('Iteration', fontsize=12)
        plt.ylabel('Î² value', fontsize=12)
        plt.title('Gradient Descent Path', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        plt.tight_layout()
        plt.show()
        
    def explain_bot_detection_application(self):
        """Explain how these concepts apply to bot detection"""
        print("ðŸ¤– 5. APPLICATION TO BOT DETECTION")
        print("-" * 45)
        print()
        
        print("ðŸ”¹ Our Feature Vector (x):")
        print("   x = [follower_ratio, tweets_per_day, account_age, verified, ...]")
        print()
        
        print("ðŸ”¹ Linear Combination (z):")
        print("   z = Î²â‚€ + Î²â‚Ã—follower_ratio + Î²â‚‚Ã—tweets_per_day + Î²â‚ƒÃ—account_age + ...")
        print()
        
        print("ðŸ”¹ Bot Probability:")
        print("   P(bot|x) = 1 / (1 + e^(-z))")
        print()
        
        print("ðŸ”¹ Training Process:")
        print("   1. Collect labeled data: (features, is_bot)")
        print("   2. Initialize parameters Î² randomly")
        print("   3. For each training example:")
        print("      â€¢ Compute predicted probability")
        print("      â€¢ Compare with true label")
        print("      â€¢ Update parameters using gradient")
        print("   4. Repeat until convergence")
        print()
        
        print("ðŸ”¹ Example Parameter Interpretation:")
        print("   If Î²â‚ = -2.5 (for follower_ratio):")
        print("   â€¢ Higher follower/following ratio â†’ lower bot probability")
        print("   â€¢ Makes sense: bots often follow many, have few followers")
        print()
        
        print("ðŸ”¹ Prediction for New Account:")
        print("   Given features: follower_ratio=0.1, tweets_per_day=50, age=30")
        print("   z = Î²â‚€ + (-2.5)Ã—0.1 + Î²â‚‚Ã—50 + Î²â‚ƒÃ—30")
        print("   P(bot) = 1 / (1 + e^(-z))")
        print()
        
        # Show actual example with realistic parameters
        self.demonstrate_bot_prediction()
        
    def demonstrate_bot_prediction(self):
        """Demonstrate actual bot prediction"""
        print("ðŸ”¹ Realistic Example:")
        print()
        
        # Realistic parameters (simplified)
        beta_0 = -1.0  # Intercept
        beta_follower_ratio = -2.0  # Negative: higher ratio = less likely bot
        beta_tweets_per_day = 0.05   # Positive: more tweets = more likely bot
        beta_age = -0.001            # Negative: older account = less likely bot
        
        # Two example accounts
        accounts = [
            {
                'name': 'Suspicious Bot',
                'follower_ratio': 0.02,    # Very low
                'tweets_per_day': 100,     # Very high
                'age_days': 30             # Very new
            },
            {
                'name': 'Normal Human',
                'follower_ratio': 2.5,     # Normal
                'tweets_per_day': 3,       # Normal
                'age_days': 1200           # Established
            }
        ]
        
        for account in accounts:
            z = (beta_0 + 
                 beta_follower_ratio * account['follower_ratio'] +
                 beta_tweets_per_day * account['tweets_per_day'] +
                 beta_age * account['age_days'])
            
            prob_bot = 1 / (1 + np.exp(-z))
            
            print(f"   {account['name']}:")
            print(f"   â€¢ Follower ratio: {account['follower_ratio']}")
            print(f"   â€¢ Tweets/day: {account['tweets_per_day']}")
            print(f"   â€¢ Age: {account['age_days']} days")
            print(f"   â€¢ z = {z:.3f}")
            print(f"   â€¢ P(bot) = {prob_bot:.1%}")
            print(f"   â€¢ Classification: {'BOT' if prob_bot > 0.5 else 'HUMAN'}")
            print()
        
    def provide_summary(self):
        """Provide a comprehensive summary"""
        print("ðŸŽ“ Key Takeaways:")
        print()
        print("1. ðŸ“Š LOGISTIC REGRESSION:")
        print("   â€¢ Maps any real number to probability [0,1] using sigmoid")
        print("   â€¢ Perfect for binary classification (bot vs human)")
        print()
        print("2. ðŸ“ˆ SIGMOID FUNCTION:")
        print("   â€¢ S-shaped curve: Ïƒ(z) = 1/(1+e^(-z))")
        print("   â€¢ Smooth, differentiable, bounded [0,1]")
        print()
        print("3. ðŸŽ¯ MAXIMUM LIKELIHOOD:")
        print("   â€¢ Find parameters that make observed data most likely")
        print("   â€¢ Maximize: â„“(Î²) = Î£[y log(p) + (1-y) log(1-p)]")
        print()
        print("4. â¬‡ï¸ GRADIENT DESCENT:")
        print("   â€¢ Iteratively update parameters: Î² â† Î² + Î±âˆ‡â„“")
        print("   â€¢ Gradient: âˆ‡â„“ = Î£(y - p)x")
        print()
        print("5. ðŸ¤– BOT DETECTION:")
        print("   â€¢ Features: follower ratios, activity patterns, account age")
        print("   â€¢ Output: probability that account is automated")
        print("   â€¢ Decision: classify as bot if P(bot) > 0.5")
        print()
        print("ðŸš€ This mathematical foundation enables our 98% accuracy!")

def main():
    """Run the complete explanation"""
    explainer = LogisticRegressionExplainer()
    explainer.explain_all_concepts()
    
    print("\n" + "="*60)
    print("ðŸ“š INTERACTIVE DEMO COMPLETE")
    print("="*60)
    print("This explanation covers the mathematical foundations")
    print("behind our bot detection system. The same principles")
    print("apply to the GUI demo you can run with:")
    print("    python bot_detector_gui.py")

if __name__ == "__main__":
    main() 